{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have the option here to user twikit instead of tweepy. I need the ability to pull trends and tweets from twitter, which is a paid feature in the offical api. Therefore, for educational purposes only, I am using twikit instead of tweepy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Jacob\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\Jacob\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from twikit import Client\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import nltk.corpus\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import datetime\n",
    "import time\n",
    "import random\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('vader_lexicon')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#username: Siftr0\n",
    "#password: 10@SiftPass\n",
    "\n",
    "# Using twikit\n",
    "# https://blog.apify.com/how-to-scrape-tweets-and-more-on-twitter-59330e6fb522/\n",
    "# https://twikit.readthedocs.io/en/latest/twikit.html\n",
    "\n",
    "client = Client('en-US')\n",
    "# client.login(auth_info_1='Siftr0', password='10@SiftPass')\n",
    "# client.save_cookies('cookies.json') #run once saved cookies - stops from hitting rate limits\n",
    "client.load_cookies(path='cookies.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_current_trends():\n",
    "    trends = client.get_trends('Trending') #rate limit 20000 every 15m\n",
    "    trend_list = []\n",
    "    for trend in trends:\n",
    "            trend_list.append(trend.name)\n",
    "    return trend_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the top 10 tweets from each trend\n",
    "\n",
    "#Cleaning text\n",
    "# https://monkeylearn.com/blog/text-cleaning/\n",
    "\n",
    "def get_tweets_from_query(query):\n",
    "    tweets = client.search_tweet(query, 'Top') #rate limit 50 every 15m\n",
    "    tweet_list = []\n",
    "    for tweet in tweets:\n",
    "        _tweet = tweet.text.lower()\n",
    "        _tweet = re.sub(r\"(@\\[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\", \"\", _tweet) #regex to remove unicode chatacters taken from monkeylearn link above\n",
    "        _tweet = \" \".join([word for word in _tweet.split() if word not in (stopwords.words('english'))]) #removes stopwords\n",
    "        tweet_list.append(_tweet)\n",
    "    return tweet_list\n",
    "\n",
    "#maybe wait a 1m between each client search - if all 40 trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sentiment analysis on each tweet text\n",
    "#averaging together setiment analysis for each trend\n",
    "\n",
    "sentiment = SentimentIntensityAnalyzer()\n",
    "\n",
    "def get_avg_sentiment(tweets):\n",
    "    tweet_sentiment = 0\n",
    "    length = 0\n",
    "    for tweet in tweets:\n",
    "        pol_score = sentiment.polarity_scores(tweet)[\"compound\"] \n",
    "        if pol_score > 0.0:\n",
    "            tweet_sentiment += pol_score\n",
    "            length += 1\n",
    "    if length == 0:\n",
    "        length = 1\n",
    "    return (tweet_sentiment / length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 trends found\n",
      "18 tweets found for Branthwaite\n",
      "19 tweets found for Dunk\n",
      "19 tweets found for Toney\n",
      "19 tweets found for D-Day\n",
      "19 tweets found for Bowen\n",
      "19 tweets found for Wolves\n",
      "19 tweets found for #Starship\n",
      "20 tweets found for Haymitch\n",
      "20 tweets found for #TheFamouslyBreathless\n",
      "19 tweets found for Quansah\n",
      "19 tweets found for Michael Mosley\n",
      "19 tweets found for Tomori\n",
      "19 tweets found for Dragon Age\n",
      "19 tweets found for #PAKvUSA\n",
      "18 tweets found for Coco\n",
      "19 tweets found for Nazis\n",
      "19 tweets found for PGMOL\n",
      "19 tweets found for Colwill\n",
      "18 tweets found for Challinor\n",
      "19 tweets found for Injured\n",
      "19 tweets found for Bosnia\n",
      "19 tweets found for Schengen\n",
      "19 tweets found for #PostOfficeInquiry\n",
      "19 tweets found for Ben White\n",
      "19 tweets found for Year 3\n",
      "19 tweets found for #ThreeLions\n",
      "19 tweets found for Macron\n",
      "19 tweets found for Bounty\n",
      "19 tweets found for Frank Hester\n",
      "6 tweets found for Toblerone\n"
     ]
    }
   ],
   "source": [
    "\n",
    "current_trends = get_current_trends()\n",
    "print(f\"{len(current_trends)} trends found\")\n",
    "trends_tweets = []\n",
    "\n",
    "for trend in current_trends:\n",
    "    tweets = get_tweets_from_query(trend)\n",
    "    time.sleep(random.randint(5,10))\n",
    "    trends_tweets.append([trend,tweets])\n",
    "    print(f\"{len(tweets)} tweets found for {trend}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "trend_data = {\n",
    "    \"Trend\" : [],\n",
    "    \"Sentiment\" : [],\n",
    "    \"Last Seen\" : []\n",
    "}\n",
    "\n",
    "for trend in trends_tweets:\n",
    "    trend_data[\"Trend\"].append(trend[0])\n",
    "    trend_data[\"Sentiment\"].append(get_avg_sentiment(trend[1]))\n",
    "    trend_data[\"Last Seen\"].append(datetime.datetime.now())\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trends common nouns\n",
    "trend_words_fdist = []\n",
    "tweet_string = ''\n",
    "for trend in trends_tweets:\n",
    "    for tweet in trend[1]:\n",
    "        tweet_string += tweet\n",
    "    tweet_tokens = nltk.word_tokenize(tweet_string)\n",
    "    fdist = nltk.FreqDist(word for word in tweet_tokens)\n",
    "    trend_words_fdist.append((trend[0], fdist))\n",
    "    tweet_string = ''\n",
    "\n",
    "\n",
    "trend_common_nouns = {\n",
    "    'Trend': [],\n",
    "    'Words': []\n",
    "}\n",
    "for trend in trend_words_fdist:\n",
    "    _trend = trend[0]\n",
    "    _fdist = trend[1]\n",
    "    common_nouns = []\n",
    "    for word in nltk.pos_tag(list(_fdist)):\n",
    "        _word = word[0]\n",
    "        _tag = word[1]\n",
    "        if _fdist[_word] > 2 and _tag == \"NN\": # threshold\n",
    "            common_nouns.append(_word)\n",
    "    trend_common_nouns['Trend'].append(_trend)\n",
    "    trend_common_nouns['Words'].append(', '.join(common_nouns))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_trend_nouns = pd.read_csv(\"Common_Words.csv\").to_dict(orient=\"list\")\n",
    "#update csv\n",
    "for trend in enumerate(trend_common_nouns['Trend']):\n",
    "    if trend[1] not in old_trend_nouns['Trend']:\n",
    "        old_trend_nouns['Trend'].append(trend[1])\n",
    "        old_trend_nouns['Words'].append(trend_common_nouns['Words'][trend[0]])\n",
    "    else:\n",
    "        index = old_trend_nouns['Trend'].index(trend[1])\n",
    "        words = old_trend_nouns['Words'][index].split(\", \")\n",
    "        _words = trend_common_nouns['Words'][trend[0]].split(\", \")\n",
    "        for word in _words:\n",
    "            if word not in words:\n",
    "                old_trend_nouns['Words'][index] += f\", {word}\"\n",
    "\n",
    "pd.DataFrame.from_dict(old_trend_nouns).to_csv(\"Common_Words.csv\", index=0,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_trends = {\n",
    "    'Trend' : [],\n",
    "    'Matches' : []\n",
    "}\n",
    "\n",
    "for words in enumerate(trend_common_nouns['Words']):\n",
    "    matches = []\n",
    "    for _words in enumerate(old_trend_nouns['Words']):\n",
    "        for word in words[1].split(', '):\n",
    "            if type(_words[1]) == str:\n",
    "                if word in _words[1].split(', ') and trend_common_nouns['Trend'][words[0]] != old_trend_nouns['Trend'][_words[0]]:\n",
    "                    matches.append(old_trend_nouns['Trend'][_words[0]])\n",
    "                    break\n",
    "    if len(matches) != 0:\n",
    "        grouped_trends['Trend'].append(trend_common_nouns['Trend'][words[0]])\n",
    "        grouped_trends['Matches'].append(', '.join(matches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read historical trends\n",
    "history = pd.read_csv(\"trends.csv\").to_dict(orient=\"list\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "returning_trends = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#update trends csv file\n",
    "\n",
    "calc_trend_data = trend_data\n",
    "\n",
    "for trend in enumerate(history[\"Trend\"]):\n",
    "    if trend[1] in calc_trend_data[\"Trend\"]: #seen trend before\n",
    "        new_trend_index = 0\n",
    "        for _trend in calc_trend_data[\"Trend\"]:\n",
    "            if _trend == trend[1]:\n",
    "                break\n",
    "            else:\n",
    "                new_trend_index += 1\n",
    "                \n",
    "        if history[\"Last Seen\"][trend[0]][:10] != str(calc_trend_data[\"Last Seen\"][new_trend_index].date()):#if not same day\n",
    "            returning_trends.append((history['Trend'][trend[0]],history[\"Last Seen\"][new_trend_index][:10]))\n",
    "            history[\"Last Seen\"][trend[0]] = calc_trend_data[\"Last Seen\"][new_trend_index] #overwrite date\n",
    "\n",
    "        history[\"Sentiment\"][trend[0]] = (history[\"Sentiment\"][trend[0]] + calc_trend_data[\"Sentiment\"][new_trend_index]) / 2 # create better sentiment avg\n",
    "\n",
    "        calc_trend_data[\"Trend\"].pop(new_trend_index)\n",
    "        calc_trend_data[\"Sentiment\"].pop(new_trend_index)\n",
    "        calc_trend_data[\"Last Seen\"].pop(new_trend_index)\n",
    "\n",
    "\n",
    "for i in range(len(calc_trend_data[\"Trend\"])):\n",
    "    history[\"Trend\"].append(calc_trend_data[\"Trend\"][i])\n",
    "    history[\"Sentiment\"].append(calc_trend_data[\"Sentiment\"][i])\n",
    "    history[\"Last Seen\"].append(calc_trend_data[\"Last Seen\"][i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new data to put in trends.csv\n",
    "new_df = pd.DataFrame.from_dict(history)\n",
    "new_df.to_csv(\"trends.csv\", index=0,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_to_words(score):\n",
    "    score = int(score * 100) \n",
    "    string = ''\n",
    "    if score in range(-5, 5):\n",
    "        string = 'neutral'\n",
    "    elif score in range(-20, -5) or score in range(5, 20):\n",
    "        string += 'slightly'\n",
    "    elif score in range(-40, -20) or score in range(20, 40):\n",
    "        string += 'somewhat'\n",
    "    elif score in range(-60, -40) or score in range(40, 60):\n",
    "        string += 'mostly'\n",
    "    elif score in range(-80, -60) or score in range(60, 80):\n",
    "        string += 'highly'\n",
    "    elif score in range(-100, -80) or score in range(80, 100):\n",
    "        string += 'extremely'\n",
    "\n",
    "    if string != 'neutral':\n",
    "        if score > 0:\n",
    "            string += ' positive'\n",
    "        else:\n",
    "            string += ' negative'\n",
    "\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Posting results on twitter\n",
    "date = datetime.datetime.now()\n",
    "cycle = int(date.strftime(\"%H\")) # So posts can be made at different times\n",
    "\n",
    "def post_opposite_trends(): #Top and bottom trend sentiment\n",
    "    df = pd.DataFrame.from_dict(trend_data)\n",
    "    top = df.sort_values(\"Sentiment\").tail(1)\n",
    "    top = pd.DataFrame.to_dict(top, orient=\"list\")\n",
    "    bottom = df.sort_values(\"Sentiment\").head(1)\n",
    "    bottom = pd.DataFrame.to_dict(bottom, orient=\"list\")\n",
    "\n",
    "    template = random.randint(1,3)\n",
    "    if template == 1:\n",
    "        tweet = f\"A lot of {sentiment_to_words(bottom['Sentiment'][0])} views radiating from the people talking about {bottom['Trend'][0]}. Perhaps the people yapping about {top['Trend'][0]} can chime in?\"\n",
    "    elif template == 2:\n",
    "        tweet = f\"Honestly, the fact everyones views are {sentiment_to_words(top['Sentiment'][0])} about {top['Trend'][0]} is quite interesting if you think about it.\"\n",
    "    elif template == 3:\n",
    "        tweet = f\"I did some analysis on {top['Trend'][0]} and {bottom['Trend'][0]}\\n\\nOn average people are {sentiment_to_words(top['Sentiment'][0])} in their tweets towards {top['Trend'][0]} and {sentiment_to_words(bottom['Sentiment'][0])} towards {bottom['Trend'][0]}\"\n",
    "        \n",
    "    client.create_tweet(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "#post any trends that have returned\n",
    "\n",
    "def post_returned_trends():\n",
    "    tweet = ''\n",
    "    if len(returning_trends) > 1:\n",
    "        tweet = f\"Hey! I've seen all this before...\\n\"\n",
    "        for trend in returning_trends: #trend = (\"trend\",\"last seen\")\n",
    "            if len(tweet) < 90:\n",
    "                tweet += f\"{trend[0]} on {trend[1]}\\n\"\n",
    "            else:\n",
    "                break\n",
    "        tweet += \"I suppose they're all important topics.\"\n",
    "    elif len(returning_trends) == 1:\n",
    "        tweet = f\"Looks like we're talking about {returning_trends[0][0]} again.\"\n",
    "\n",
    "    if tweet != '':\n",
    "        client.create_tweet(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_all_time_ranks():\n",
    "    all_bottom = new_df.sort_values(\"Sentiment\").head(3)\n",
    "    all_bottom = pd.DataFrame.to_dict(all_bottom, orient=\"list\")\n",
    "    all_top = new_df.sort_values(\"Sentiment\").tail(3)\n",
    "    all_top = pd.DataFrame.to_dict(all_top, orient=\"list\")\n",
    "\n",
    "    template = random.randint(1,3)\n",
    "    if template == 1:\n",
    "        tweet = f\"Isn't it fascinating that {all_top['Trend'][0]}, {all_top['Trend'][1]} and {all_top['Trend'][2]} get the most positve views! I could talk about it all day!\"\n",
    "    elif template == 2:\n",
    "        tweet = f\"It appears {all_bottom['Trend'][0]}, {all_bottom['Trend'][1]} and {all_bottom['Trend'][2]} are the lowest of the low in terms of sentiment. I wonder why?\"\n",
    "    elif template == 3:\n",
    "        tweet = f\"People talk highly of {all_top['Trend'][0]}, {all_top['Trend'][1]} and {all_top['Trend'][2]} but so lowly of {all_bottom['Trend'][0]}, {all_bottom['Trend'][1]} and {all_bottom['Trend'][2]}. Anyone have any thoughts on this?\"\n",
    "        \n",
    "    client.create_tweet(tweet)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_linked_trends():\n",
    "    index = random.randint(0,len(grouped_trends['Trend'])-1)\n",
    "    links = grouped_trends['Matches'][index].split(\", \")\n",
    "\n",
    "    template = random.randint(1,3)\n",
    "    if template == 1:\n",
    "        tweet = f\"For anyone interested in {grouped_trends['Trend'][index]}, you might also be interested in {links[random.randint(0,len(links))-1]}\"\n",
    "    elif template == 2:\n",
    "        if len(links) != 1:\n",
    "            tweet = f\"0_0\\n\\n {grouped_trends['Trend'][index] }\"\n",
    "            for trend in enumerate(links):\n",
    "                if trend[0] == len(links)-1:\n",
    "                    tweet += f\"and {trend[1]} are all connected...\"\n",
    "                else:\n",
    "                    tweet += f\"{trend[1]}, \"\n",
    "        else:\n",
    "            tweet = f\"0_0\\n\\n{grouped_trends['Trend'][index]} and {links[0]} are connected...\"\n",
    "    elif template == 3:\n",
    "        tweet = f\"Ever think about how {grouped_trends['Trend'][index]} and {links[random.randint(0,len(links))-1]} are linked?\"\n",
    "        \n",
    "    client.create_tweet(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if date.day % 7 == 0 and cycle == 18:\n",
    "    post_all_time_ranks()\n",
    "    time.sleep(10)\n",
    "\n",
    "if cycle % 4 == 0:\n",
    "    if random.randint(1,2) == 2:\n",
    "        post_opposite_trends()\n",
    "    else:\n",
    "        post_linked_trends()\n",
    "\n",
    "time.sleep(10)\n",
    "post_returned_trends()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "big-data-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
