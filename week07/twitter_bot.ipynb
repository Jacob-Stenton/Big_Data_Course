{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95a93b61",
   "metadata": {},
   "source": [
    "# Making A Twitter Bot: Part 1\n",
    "\n",
    "Today we will also be using the [Tween library](https://www.tweepy.org/) to access the twitter API, You can also have a look at this the [tutorial](https://dev.to/twitterdev/a-comprehensive-guide-for-using-the-twitter-api-v2-using-tweepy-in-python-15d9#:~:text=If%20you%20want%20to%20create,with%20Read%20and%20Write%20permissions) for guidance n the library.\n",
    "\n",
    "There have been some changes to the Twitter API will work you thorough how to get [free access]( https://developer.twitter.com/en/docs/twitter-api/getting-started/about-twitter-api)  via"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13ced36",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4947501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping Python imports\n",
    "# 1. standard (built-in) libraries - the ones we don't need to install\n",
    "from io import BytesIO\n",
    "\n",
    "# 2. third-party libraries (the ones we install using pip or conda)\n",
    "import freetype as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import requests\n",
    "import tweepy\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from PIL import Image\n",
    "\n",
    "# 3. custom libraries (from our own files)\n",
    "import config_marysia as config # EDIT WITH YOUR OWN CONFIG FILE NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91b3486",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "The access tokens and keys below come from your developer portal at [developer.twitter.com](https://developer.twitter.com/en). and have been entered in the config file of the week's project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b69138",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# important: set the app permissions first, then regenerate the access tokens!\n",
    "client = tweepy.Client(\n",
    "    bearer_token=config.bearer_token,\n",
    "    consumer_key=config.api_key,\n",
    "    consumer_secret=config.api_key_secret,\n",
    "    access_token=config.access_token,\n",
    "    access_token_secret=config.access_token_secret,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329ca41e-7a86-4cf4-b7ef-fddba5b14c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweepy.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25ec05b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Adding a tweet using the Tween Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ee707b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# NOTE: if this does not work in VSCode, try running the notebook in the browser\n",
    "response = client.create_tweet(text='Hello world :)')\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d0fafb",
   "metadata": {},
   "source": [
    "## Recap on methods to create Ascii Art"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f29b064",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sorted_chars(text):\n",
    "    ## converts the raw text to a set of characters\n",
    "    no_repeats = set(text)\n",
    "    # Loads the font file from the folder. Make sure the file is also in the my-practice folder\n",
    "    # where you are running your own notebook\n",
    "    f = F.Face('Ubuntu-M.ttf')\n",
    "    f.set_char_size(99)\n",
    "    sorted_chars = sorted([(f.load_char(c) or sum(f.glyph.bitmap.buffer) ,c)for c in no_repeats])\n",
    "    return sorted_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3518ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_img(url):\n",
    "     # sample info about user agent, which Wikipedia requires (though it's not always executed)\n",
    "    headers = {'User-Agent': 'CoolBot/0.0 (https://example.org/coolbot/; coolbot@example.org)'}\n",
    "    response = requests.get(url, headers=headers, stream=True)\n",
    "    return Image.open(BytesIO(response.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b820d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_2_ascii(img, chars):\n",
    "    step = 1 / len(chars)\n",
    "    grayscale = img.convert(mode=\"L\")\n",
    "    # unfortunately the size can't be bigger due to tweet lentgth limit\n",
    "    resized = grayscale.resize(size=(20, 12), resample=Image.Resampling.NEAREST)\n",
    "    resized_array = np.array(resized)\n",
    "    ascii_img = []\n",
    "    \n",
    "    for row in resized_array:\n",
    "        ascii_row = []\n",
    "        for col in row:\n",
    "            scaled = col / 255\n",
    "            # mapping the pixel value to the character index in the\n",
    "            idx = max(0, int(scaled / step) - 1)\n",
    "            ascii_row.append(chars[idx][1])\n",
    "        ascii_img.append(ascii_row)\n",
    "    return ascii_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9911aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = download_img('https://pbs.twimg.com/media/FSUvik-XIAE4lxJ.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d584210a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e85b82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8686492e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_chars = get_sorted_chars(\"This }{;',.?'} is some 234 rando&m text w1th s0m3 num83rs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5de2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ascii_img = img_2_ascii(img, sorted_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52adf37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_ascii_img(ascii_img):\n",
    "    for row in ascii_img:\n",
    "        print(\"\".join(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c3e3c9",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "show_ascii_img(ascii_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5d0328",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Recap we are going to scrape Wikipedia for Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f863b5b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def link_is_valid(link):\n",
    "    required_types = ['png', 'jpg', 'jpeg']\n",
    "    if link.startswith('/wiki/') or link.startswith('//upload.wikimedia.org') :\n",
    "        valid = False\n",
    "        for type in required_types:\n",
    "            if type  in link:\n",
    "                valid = True\n",
    "                break\n",
    "    return valid\n",
    "\n",
    "\n",
    "def get_links(soup):\n",
    "    links = []\n",
    "    for a in soup.find_all(\"a\"):\n",
    "        try:\n",
    "            link = a[\"href\"]\n",
    "            if link_is_valid(link):\n",
    "                links.append(link)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    return links\n",
    "\n",
    "def get_picture_links(soup):\n",
    "    links = []\n",
    "    for div in soup.find_all(\"div\",class_=\"fullImageLink\"):\n",
    "        children = div.findChildren(\"a\" , recursive=False)\n",
    "        for child in children:\n",
    "            #print(child)\n",
    "            try:\n",
    "                link = child[\"href\"]\n",
    "                if link_is_valid(link):\n",
    "                    links.append(link)\n",
    "            except:\n",
    "                pass\n",
    "    return links"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c9ea30",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Web Scraping Recap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4092bfc4",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    " We are going to web scrap wikipedia given a word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3cbecc",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "query_word = 'cat'\n",
    "url = 'https://en.wikipedia.org/wiki/' + query_word\n",
    "page = requests.get(url)\n",
    "soup = BeautifulSoup(page.content, features=\"html.parser\")\n",
    "links = list(set(get_links(soup)))\n",
    "print(links)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a054af3a",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In the below we are going to get all the paragraph tag text associated with the wiki page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33d1ac6",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "p_tags = soup.find_all(['p'])\n",
    "html_p_tag_text = ' '.join([p_tag.text for p_tag in p_tags])\n",
    "# clean text\n",
    "escapes = ''.join([chr(char) for char in range(1, 32)])\n",
    "translator = str.maketrans('', '', escapes)\n",
    "cleaned_html_p_tag_text = html_p_tag_text.translate(translator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87140d5",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We are then going to find the source of all the images on the wiki page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5baee0c6",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_image_from_url(url):\n",
    "    page = requests.get(url, stream=True)\n",
    "    soup = BeautifulSoup(page.content, features=\"html.parser\")\n",
    "    picture_links = get_picture_links(soup)\n",
    "    return picture_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93b1948",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "all_picture_links = []\n",
    "for l in links:\n",
    "    link_url = 'https://en.wikipedia.org' + l\n",
    "    all_picture_links.extend(get_image_from_url(link_url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c54eed",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "all_picture_links_cleaned = []\n",
    "for  l in all_picture_links:\n",
    "    all_picture_links_cleaned.append('https:' + l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4629c7",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_picture_links_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552d65b1",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "we are then going to pic a random picture and convert to ascii image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382b8c26-d84d-4438-a858-d2555d614e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_url = all_picture_links_cleaned[8]\n",
    "img = download_img(img_url)\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b583cc-3e20-4254-b707-a807913cc2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_chars = get_sorted_chars(cleaned_html_p_tag_text)\n",
    "ascii_img = img_2_ascii(img, sorted_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de494153",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ascii_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68765cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_ascii_img(ascii_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e241bd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ascii_2_tweet(ascii_img):\n",
    "    rows = [\"\".join(row) for row in ascii_img]\n",
    "    return \"\\n\".join(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4899ef0",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ascii_2_tweet(ascii_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18626107",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = ascii_2_tweet(ascii_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ae1698",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.create_tweet(text=tweet)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033a21cc-c822-401e-949e-ffb00c3d0a33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
