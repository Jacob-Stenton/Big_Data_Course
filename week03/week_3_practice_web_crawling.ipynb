{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e8aaf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68fb21d0",
   "metadata": {},
   "source": [
    "# Recaps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8610d4d",
   "metadata": {},
   "source": [
    "## HTTP\n",
    "\n",
    "A [Hypertext Transfer Protocol (HTTP)](https://en.wikipedia.org/wiki/HTTP) request is made by a client, to a named host, which is located on a server.\n",
    "\n",
    "A HTTP request contains the following elements:\n",
    "- A request line.\n",
    "- A series of HTTP headers, or header fields.\n",
    "- A message body, if needed. which is usually a JavaScript [Object Notation (JSON)](https://www.w3schools.com/js/js_json_objects.asp)\n",
    "\n",
    "HTTP request methods\n",
    "GET\n",
    "POST\n",
    "PUT\n",
    "DELETE\n",
    "\n",
    "[and more ...](https://developer.mozilla.org/en-US/docs/Web/HTTP/Methods)\n",
    "\n",
    "Example Request\n",
    "<img src=\"https://developer.mozilla.org/en-US/docs/Web/HTTP/Overview/http_request.png\"/>\n",
    "\n",
    "Example Reponse\n",
    "<img src=\"https://developer.mozilla.org/en-US/docs/Web/HTTP/Overview/http_response.png\"/>\n",
    "\n",
    "The HTTP Response will have :\n",
    "- A [status code](https://developer.mozilla.org/en-US/docs/Web/HTTP/Status), indicating if the request was successful or not, and why.\n",
    "- A status message, a non-authoritative short description of the status code.\n",
    "- HTTP headers, like those for requests.\n",
    "- Optionally, a body containing the fetched resource which can be a blob or a JSON object aswell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0463abc3",
   "metadata": {},
   "source": [
    "## Databases\n",
    "\n",
    "<img src=\"https://images.prismic.io/nightborn/7e215705-8aa6-4ff4-94bf-5a6a801b92a4_thumbnail_website2.jpg?auto=compress,format\"/>\n",
    "\n",
    "One of the most common databases is a :\n",
    "\n",
    "<b>Relational Database management system (RDMS) or simply relation Databases:</b> is a collection that organizes data in predefined relationships where data is stored in one or more tables columns and rows\n",
    "- Structured Query Language(SQL), is a domain-specific language used in programming and managing of data held in a relational database. https://www.w3schools.com/sql/\n",
    "\n",
    "Some examples include\n",
    "- Microsoft SQL Server\n",
    "- MySQL\n",
    "- PostgreSQL\n",
    "- SQLite\n",
    "- Oracle Database\n",
    "- MariaDB\n",
    "\n",
    "\n",
    "<b>[NoSQL](https://www.couchbase.com/resources/why-nosql/) database:</b>  stores information in JSON documents instead of columns and rows used by relational databases. NoSQL stands for “not only SQL” rather than “no SQL” at all. Types of NoSql Databases include\n",
    "- Document databases\n",
    "- Key-value stores\n",
    "- Wide-column databases\n",
    "- Graph databases\n",
    "\n",
    "Some examples include:\n",
    "- MongoDB\n",
    "- Apache Cassandra\n",
    "- Couchbase\n",
    "- Amazon DynamoDB\n",
    "- Redis\n",
    "- Neo4j"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5bf5bd8",
   "metadata": {},
   "source": [
    "# Web Crawling\n",
    "\n",
    "The goal of this notebook is to go to a Wikipedia page,\n",
    " - scrape all the links from this page,\n",
    "  - store them somehow\n",
    "  - pick a link at random to perform the same process again.\n",
    "This will repeat for a set number of iterations, but could run infinitely in theory.. until something crashes.\n",
    "\n",
    "This is to introduce you to some concepts behind web-crawling - perhaps it will stimulate some ideas about how you might make a more directed and intentional web-crawler with some specific goal in mind...\n",
    "\n",
    "The first thing to do is get an idea of what we are working with. Going to some random Wikipedia article (in this case.. chicken) we can see a few things:\n",
    "\n",
    "\n",
    "1. The URLs are of the form `https://en.wikipedia.org/wiki/Chicken` - which is convenient and neat.\n",
    "2. The HTML markup also seems quite neat and links are in clear `<a>` tags.\n",
    "3. Links to other Wikipedia articles seem to be of the shortened form `/wiki/Domestication` (as seen in the pic).\n",
    "4. There are _a lot_ of links on a page.\n",
    "\n",
    "> If you didn't know, all I did in the pic above is just right click on a link and click `Inspect` - this opens up the console and expands the HTML to reveal the specific tag you are _inspecting_. This is really handy. I'm using Firefox but I'd imagine all good browsers (...Firefox or Chrome) have this feature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eabf6f6",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "\n",
    "As with any task, you need to break it down into manageable chunks as soon as possible. First of all I want to make sure I can make a HTTP request to a Github page and retrieve the page content.\n",
    "\n",
    "I'll be using the [Requests](https://docs.python-requests.org/en/latest/) package today, but [URLLib](https://docs.python.org/3/library/urllib.html) would also work. They pretty much do the same thing, but it seems Requests is used more often and is a slightly nicer package to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5968421",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'chicken'\n",
    "\n",
    "url = 'https://en.wikipedia.org/wiki/' + query\n",
    "page = requests.get(url)\n",
    "soup = BeautifulSoup(page.content, features=\"html.parser\")\n",
    "print(soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457f416f",
   "metadata": {},
   "source": [
    "That seemed to work fine. Notice I had to run BeautifulSoup on `page.content`, because `page` itself is just the HTTP response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8c6ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(page))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39016ea",
   "metadata": {},
   "source": [
    "Here we have a status code of `200` meaning the request is successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f94485a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(page.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecbb069f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for method in dir(page):\n",
    "    print(method)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18017a21",
   "metadata": {},
   "source": [
    "Let's do some BeautifulSoup magic and grab all the links from the page. A HTML link is always in a `<a>` tag, and specifically is under the `href` attribute of the `<a>` tag.\n",
    "\n",
    "I am putting this in a `try` `except` block here as_not every_ `<a>` tag will necessarily have an `href` attribute. Try running:\n",
    "\n",
    "```python\n",
    "links = []\n",
    "\n",
    "for a in soup.find_all(\"a\"):\n",
    "    links.append(a[\"href\"])\n",
    "```\n",
    "\n",
    "You'll get an error and the whole loop is ruined. Getting used to when and where to use `try` `except` isn't always obvious, but it is a way of _catching_ an error, handling it in some way, and then _continuing_ as opposed to simply crashing :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e516aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "links = []\n",
    "\n",
    "for a in soup.find_all(\"a\"):\n",
    "    try:\n",
    "        links.append(a[\"href\"])\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "for link in links:\n",
    "    print(link)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028c37cc",
   "metadata": {},
   "source": [
    "This is a great start! You can see we have _lots_ of internal Wikipedia links (links to other Wikipedia articles).. This could be the start of a Wikipedia specific crawler.\n",
    "\n",
    "The links which start with a `#` are references to breakpoints on the page, so that you could send the link to someone already scrolled to a specific point on the page.\n",
    "\n",
    "Now I want to just grab all the Wikipedia links and filter everything else out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ed0f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered = [link for link in links if link.startswith('/wiki/')]\n",
    "\n",
    "for f in filtered:\n",
    "    print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c95661",
   "metadata": {},
   "source": [
    "That got all internal Wikipedia links, but there are also images (`.jpg`, `.png`, `.tif`) in there. There are also a bunch of other things so I am gonna do that again. The cell below does the same check above, but then also checks if any of the other junk I don't want is in the link, and of course skips it if so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d39f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "ignores = ['png', 'jpg', 'jpeg', 'isbn', 'svg', 'identifier', \\\n",
    "           'File', 'Special', 'Template', 'Mailto', 'Portal', \\\n",
    "           'Help', 'Category', 'Talk', 'Wikipedia', 'Main_Page']\n",
    "\n",
    "filtered = []\n",
    "\n",
    "for link in links:\n",
    "    if link.startswith('/wiki/'):\n",
    "        valid = True\n",
    "        for ignore in ignores:\n",
    "            if ignore in link:\n",
    "                valid = False\n",
    "                break\n",
    "        if valid:\n",
    "            filtered.append(link)\n",
    "\n",
    "for f in filtered:\n",
    "    print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363d1be4",
   "metadata": {},
   "source": [
    "An alternative way to find links with a particular pattern would be to use [regular expression](https://www.regular-expressions.info/#:~:text=A%20regular%20expression%20(regex%20or,with%20wildcard%20notations%20such%20as%20*)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da784f9",
   "metadata": {},
   "source": [
    "#### <span style=\"color:red\"> Exercise 1 </span>\n",
    "\n",
    "practice using beautiful soup and nltk get all the text html tag in the chicken page and count the number of time the word \"chicken\" is used There should be about ~ 184 occurrences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0d18ad",
   "metadata": {},
   "source": [
    "#### <span style=\"color:red\"> Exercise 2 </span>\n",
    "\n",
    "Go over this [python doc](https://docs.python.org/3/library/re.html) and use regular expresssions to grab the links that have article. How does this improve the code?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8e9307",
   "metadata": {},
   "source": [
    "We now have a good number of valid links to other Wikipedia articles..\n",
    "\n",
    "Now we can really start crawling! For now lets just choose a link at random and then see what that Wikipedia article has for us..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0671832",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_wiki = random.choice(filtered)\n",
    "url = url = 'https://en.wikipedia.org' + random_wiki\n",
    "page = requests.get(url)\n",
    "soup = BeautifulSoup(page.content, features=\"html.parser\")\n",
    "\n",
    "print(f\"URL: {url}\")\n",
    "\n",
    "new_links = []\n",
    "\n",
    "for a in soup.find_all(\"a\"):\n",
    "    try:\n",
    "        new_links.append(a[\"href\"])\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "new_filtered = []\n",
    "\n",
    "for link in new_links:\n",
    "    if link.startswith('/wiki/'):\n",
    "        valid = True\n",
    "        for ignore in ignores:\n",
    "            if ignore in link:\n",
    "                valid = False\n",
    "                # As soon as we know the link is not valid, there's is no point\n",
    "                # checking the rest of the ignores, so we break:\n",
    "                break\n",
    "        if valid:\n",
    "            new_filtered.append(link)\n",
    "            \n",
    "for f in new_filtered:\n",
    "    print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce457d66",
   "metadata": {},
   "source": [
    "This is OK.. but we are just repeating ourselves (it isn't very [DRY](... add link to DRY ...)), and this doesn't really set us up to automate the process.\n",
    "\n",
    "Let's turn these mini routines into functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d00c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_soup(wiki_suffix):\n",
    "    url = url = 'https://en.wikipedia.org' + wiki_suffix\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, features=\"html.parser\")\n",
    "    \n",
    "    return soup\n",
    "\n",
    "def link_is_valid(link):\n",
    "    ignores = ['png', 'jpg', 'jpeg', 'isbn', 'svg', 'identifier', \\\n",
    "           'File', 'Special', 'Template', 'Mailto', 'Portal', \\\n",
    "           'Help', 'Category', 'Talk', 'Wikipedia', 'Main_Page']\n",
    "    \n",
    "    if link.startswith('/wiki/'):\n",
    "        valid = True\n",
    "        for ignore in ignores:\n",
    "            if ignore in link:\n",
    "                valid = False\n",
    "                break\n",
    "    return valid\n",
    "    \n",
    "\n",
    "def get_links(soup):\n",
    "    links = []\n",
    "    for a in soup.find_all(\"a\"):\n",
    "        try:\n",
    "            link = a[\"href\"]\n",
    "            if link_is_valid(link):\n",
    "                links.append(link)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    return links"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9206df",
   "metadata": {},
   "source": [
    "#### <span style=\"color:red\"> Exercise 3 </span>\n",
    "\n",
    "Add a function that given a word in counts how many times it appears in page"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50cd18a2",
   "metadata": {},
   "source": [
    "We have compartmentalised the few small routines in the code above and _abstracted_ them away into functions which are concise and are pretty much self-explanatory by their function names. This is starting to feel much nicer.. And something we could begin to turn into _software_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0fb07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_wiki = random.choice(filtered)\n",
    "soup = get_soup(random_wiki)\n",
    "links = get_links(soup)\n",
    "\n",
    "for l in links:\n",
    "    print(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec730f2e",
   "metadata": {},
   "source": [
    "And finally we can automate the process. This is pretty simple really, we just do it a bunch of times!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b276ea3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl(seed):\n",
    "    links_visited = []\n",
    "    suffix = '/wiki/' + seed\n",
    "    # we don't want this to run forever so we only navigate 10 down\n",
    "    for i in range(10):\n",
    "        soup = get_soup(suffix)\n",
    "        links = get_links(soup)\n",
    "        suffix = random.choice(links)\n",
    "        links_visited.append(suffix)\n",
    "    return links_visited\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021873cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "links_visited = crawl('soup')\n",
    "\n",
    "for lv in links_visited:\n",
    "    print(f\"Visited: {lv}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d20946a",
   "metadata": {},
   "source": [
    "#### <span style=\"color:red\"> Exercise 4 </span>\n",
    "\n",
    "Try the above with a new seed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81597d26",
   "metadata": {},
   "source": [
    "## Homework\n",
    "\n",
    "Your task now is to do _pretty much_ what I have done in this notebook, but with another source as your starting point.\n",
    "\n",
    "To take it futher try and find a more meaningful direction in your crawling.  Perhaps you could actually read what words are in the link, or in the page or find something which would allow your web-crawler to make a decision about _where_ it would like to go to next.\n",
    "\n",
    "It would also be great to _store this journey. [Perhaps some more metadata into a text file](https://www.w3schools.com/python/python_file_write.asp).\n",
    "\n",
    "Perhaps the web-crawler doesn't only move forward but can turn back (return to an older link) and start a new path. How would you record this journey?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
